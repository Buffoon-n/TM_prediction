---
alg: CONV_LSTM
base_dir: logs/pretrained/Abilene
log_level: INFO
gpu: 1
mon_ratio: 0.2
data:
  batch_size: 512
  raw_dataset_dir: Dataset/Abilene2d.npy
  eval_batch_size: 1
  data_name: Abilene
  day_size: 288
  generate_data: False
  data_size: 1.0
model:
  cl_decay_steps: 2000
  horizon: 3
  l1_decay: 0
  num_nodes: 144
  n_rnn_layers: 2
  output_dim: 1
  seq_len: 36
  wide: 12
  high: 12
  channel: 2
  filters: [2, 4]
  kernel_size: [[3, 3], [3, 3]]
  strides: [[1, 1], [1, 1]]
train:
  base_lr: 0.01
  rnn_dropout: 0.5
  conv_dropout: 0.5
  epoch: 64
  epochs: 50000
  epsilon: 0.001
  global_step: 24375
  lr_decay_ratio: 0.1
  max_grad_norm: 5
  max_to_keep: 100
  min_learning_rate: 2.0e-06
  optimizer: adam
  patience: 50
  test_every_n_epochs: 10
  results_name: Abilene-dgc-lstm
  continue_train: False
test:
  run_times: 50
  flow_selection: Random
  test_size: 2