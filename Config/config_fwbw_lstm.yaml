---
alg: FWBW-LSTM
base_dir: logs/pretrained/Abilene
log_level: INFO
gpu: 1
mon_ratio: 0.3
mode: train
data:
  batch_size: 512
  raw_dataset_dir: Dataset/Abilene2d.npy
  dataset_dir: Dataset/Abilene2d/fwbw_lstm/seq_36/
  graph_pkl_filename: Dataset/Abilene2d/fwbw_lstm/seq_36/Abilene-adj-mx
  test_batch_size: 64
  data_name: Abilene
  day_size: 288
  generate_data: True
  data_size: 1.0
model:
  cl_decay_steps: 2000
  filter_type: dual_random_walk
  horizon: 3
  input_dim: 2
  l1_decay: 0
  max_diffusion_step: 2
  num_nodes: 144
  num_rnn_layers: 2
  output_dim: 1
  rnn_units: 64
  seq_len: 36
  use_curriculum_learning: true
  r: 2
train:
  base_lr: 0.01
  dropout: 0
  epoch: 64
  epochs: 50000
  epsilon: 0.001
  global_step: 24375  
  lr_decay_ratio: 0.1
  max_grad_norm: 5
  max_to_keep: 100
  min_learning_rate: 2.0e-06
  optimizer: adam
  patience: 20
  steps:
    - 20
    - 30
    - 40
    - 50
  test_every_n_epochs: 10
  results_path: results/
  results_name: Abilene-dgc-lstm
  continue_train: False
  log_dir: logs/pretrained/Abilene/fwbw_lstm_DR_2_h_3_64-64_lr_0.01_bs_512_0822161809/
test:
  results_path: results/dgclstm-abilene/models-5087771.0000-22725.data-00000-of-00001/results
  run_times: 50
  flow_selection: Random
  lamda_0: 2.6
  lamda_1: 1.0
  lamda_2: 1.0
